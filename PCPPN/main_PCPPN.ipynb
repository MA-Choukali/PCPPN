{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4396,"status":"ok","timestamp":1709339794043,"user":{"displayName":"MJ Lab","userId":"12526651074320095318"},"user_tz":-210},"id":"RRv3I6vpu8AX","outputId":"a37eeba7-af89-4a1d-fe58-3e4e5b86168e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/PCPPN\n"]},{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":1}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd 'drive/My Drive/PCPPN'\n","import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device\n"]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"A93zDE7k1T9G"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5782,"status":"ok","timestamp":1709339804371,"user":{"displayName":"MJ Lab","userId":"12526651074320095318"},"user_tz":-210},"id":"866ZRG0Gy_Kp","outputId":"846a81d0-ea83-414d-b76b-34d502118d54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: helpers in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"]}],"source":["pip install helpers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3390,"status":"ok","timestamp":1709339807758,"user":{"displayName":"MJ Lab","userId":"12526651074320095318"},"user_tz":-210},"id":"3ReFiFjRvaqR","outputId":"36bde577-c054-46d3-cd23-1b5006bca587"},"outputs":[{"output_type":"stream","name":"stdout","text":["(16, 128, 1, 1)\n","8\n"]}],"source":["target_test_accu = 0.01\n","\n","import os\n","import shutil\n","\n","import torch\n","import torch.utils.data\n","# import torch.utils.data.distributed\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","import argparse\n","import re\n","\n","from helpers import makedir\n","import model\n","import push\n","import prune\n","import train_and_test as tnt\n","import save\n","from log import create_logger\n","from preprocess import mean, std, preprocess_input_function\n","\n","from settings import base_architecture, img_size, prototype_shape, num_classes, \\\n","                     prototype_activation_function, add_on_layers_type, experiment_run\n","print(prototype_shape)\n","print(num_classes)"]},{"cell_type":"markdown","source":["## Load Data"],"metadata":{"id":"QLqf77ew1Xac"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38393,"status":"ok","timestamp":1709339846147,"user":{"displayName":"MJ Lab","userId":"12526651074320095318"},"user_tz":-210},"id":"eQuDeMFQsyhy","outputId":"850fdc10-2a4f-4731-c8e0-cb7f79a297c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["training set size: 9600\n","push set size: 995\n","test set size: 605\n","batch size: 80\n"]}],"source":["#epoches_accuracy_saved = [i for i in range(1000) if i % 5 == 0] # show the epches to the accuracy logs be saved\n","# book keeping namings and code\n","\n","from settings import n_fold\n","\n","base_architecture_type = re.match('^[a-z]*', base_architecture).group(0)\n","\n","model_dir = './saved_models'+'_'+'F'+str(n_fold)+'_'+str(num_classes)+'/' + base_architecture + '/' + experiment_run + '/'\n","makedir(model_dir)\n","\n","# shutil.copy(src=os.path.join(os.getcwd(), __file__), dst=model_dir)  # I think this line should not be implemented (__file__ doesn't exist!)\n","shutil.copy(src=os.path.join(os.getcwd(), 'settings.py'), dst=model_dir)\n","shutil.copy(src=os.path.join(os.getcwd(), base_architecture_type + '_features.py'), dst=model_dir)\n","shutil.copy(src=os.path.join(os.getcwd(), 'model.py'), dst=model_dir)\n","shutil.copy(src=os.path.join(os.getcwd(), 'train_and_test.py'), dst=model_dir)\n","\n","log, logclose = create_logger(log_filename=os.path.join(model_dir, 'train.log'))\n","img_dir = os.path.join(model_dir, 'img')\n","makedir(img_dir) # Directory for saving prototypes\n","weight_matrix_filename = 'outputL_weights'\n","prototype_img_filename_prefix = 'prototype-img'\n","prototype_self_act_filename_prefix = 'prototype-self-act'\n","proto_bound_boxes_filename_prefix = 'bb'\n","\n","# load the data\n","from settings import train_dir, test_dir, train_push_dir, \\\n","                     train_batch_size, test_batch_size, train_push_batch_size\n","\n","# test_dir = train_push_dir\n","normalize = transforms.Normalize(mean=mean,\n","                                 std=std)\n","\n","# all datasets\n","# train set\n","\n","train_dataset = datasets.ImageFolder(\n","    train_dir,\n","    transforms.Compose([\n","        transforms.Resize(size=(img_size, img_size)),\n","        transforms.ToTensor(),\n","        normalize,\n","    ]))\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset , batch_size=train_batch_size, shuffle=True,\n","    num_workers=4, pin_memory=False)\n","\n","# push set\n","train_push_dataset = datasets.ImageFolder(\n","    train_push_dir,\n","    transforms.Compose([\n","        transforms.Resize(size=(img_size, img_size)),\n","        transforms.ToTensor(),\n","    ]))\n","train_push_loader = torch.utils.data.DataLoader(\n","    train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n","    num_workers=4, pin_memory=False)\n","\n","# test set\n","test_dataset = datasets.ImageFolder(\n","    test_dir,\n","    transforms.Compose([\n","        transforms.Resize(size=(img_size, img_size)),\n","        transforms.ToTensor(),\n","        normalize,\n","    ]))\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=test_batch_size, shuffle=False,\n","    num_workers=4, pin_memory=False)\n","\n","# we should look into distributed sampler more carefully at torch.utils.data.distributed.DistributedSampler(train_dataset)\n","log('training set size: {0}'.format(len(train_loader.dataset)))\n","log('push set size: {0}'.format(len(train_push_loader.dataset)))\n","log('test set size: {0}'.format(len(test_loader.dataset)))\n","log('batch size: {0}'.format(train_batch_size))"]},{"cell_type":"markdown","source":["## Train Model"],"metadata":{"id":"gLaYg3CK1cO0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAI2qszs5ZK1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709321447749,"user_tz":-210,"elapsed":939375,"user":{"displayName":"MJ Lab","userId":"12526651074320095318"}},"outputId":"e22bdec3-7eb1-4352-88ca-32181291fcaf"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["training set size: 9600\n","push set size: 995\n","test set size: 605\n","batch size: 80\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to ./pretrained_models/vgg19-dcbb9e9d.pth\n","100%|██████████| 548M/548M [00:02<00:00, 218MB/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["start training\n","epoch: \t0\n","\twarm\n","\ttrain\n","\ttime: \t1121.6088812351227\n","\tcross ent: \t2.7016143759091693\n","\tcluster: \t3.315107613305251\n","\tseparation:\t2.5213141376773516\n","\tavg separation:\t3.5812159637610117\n","\taccu: \t\t12.28125%\n","\tloss: \t5.163995480537414\n","\tl1: \t\t136.0\n","\tp dist pair: \t4.16764497756958\n","\ttest\n","\ttime: \t100.7240731716156\n","\tcross ent: \t2.815673657826015\n","\tcluster: \t0.872805757181985\n","\tseparation:\t0.3090490017618452\n","\tavg separation:\t1.0081316658428736\n","\taccu: \t\t10.743801652892563%\n","\tloss: \t3.501194408961705\n","\tl1: \t\t136.0\n","\tp dist pair: \t4.16764497756958\n","epoch: \t1\n","\twarm\n","\ttrain\n","\ttime: \t105.04874348640442\n","\tcross ent: \t2.4338632484277087\n","\tcluster: \t0.3780346568673849\n","\tseparation:\t0.19001649164905152\n","\tavg separation:\t0.6726459364096323\n","\taccu: \t\t21.5%\n","\tloss: \t2.7330897291501364\n","\tl1: \t\t136.0\n","\tp dist pair: \t2.163562774658203\n","\ttest\n","\ttime: \t9.51222276687622\n","\tcross ent: \t2.789089100701468\n","\tcluster: \t0.3541971913405827\n","\tseparation:\t0.1254606917500496\n","\tavg separation:\t0.4617058038711548\n","\taccu: \t\t10.082644628099173%\n","\tloss: \t3.0744100298200334\n","\tl1: \t\t136.0\n","\tp dist pair: \t2.163562774658203\n","epoch: \t2\n","\twarm\n","\ttrain\n","\ttime: \t106.229318857193\n","\tcross ent: \t2.236849037806193\n","\tcluster: \t0.18143683895468712\n","\tseparation:\t0.10159886411080758\n","\tavg separation:\t0.3896221193174521\n","\taccu: \t\t28.447916666666668%\n","\tloss: \t2.3858706831932066\n","\tl1: \t\t136.0\n","\tp dist pair: \t1.5428717136383057\n","\ttest\n","\ttime: \t9.712599039077759\n","\tcross ent: \t2.863278388977051\n","\tcluster: \t0.2519687350307192\n","\tseparation:\t0.0810590780207089\n","\tavg separation:\t0.3269688997949873\n","\taccu: \t\t9.586776859504132%\n","\tloss: \t3.0703687327248708\n","\tl1: \t\t136.0\n","\tp dist pair: \t1.5428717136383057\n","epoch: \t3\n","\twarm\n","\ttrain\n","\ttime: \t106.06821918487549\n","\tcross ent: \t2.084584532181422\n","\tcluster: \t0.1250634460399548\n","\tseparation:\t0.07715675725291173\n","\tavg separation:\t0.30035416012008986\n","\taccu: \t\t33.958333333333336%\n","\tloss: \t2.190462848544121\n","\tl1: \t\t136.0\n","\tp dist pair: \t1.2531322240829468\n","\ttest\n","\ttime: \t9.629244089126587\n","\tcross ent: \t2.8508401598249162\n","\tcluster: \t0.20837936231068202\n","\tseparation:\t0.06996052180017744\n","\tavg separation:\t0.2764750208173479\n","\taccu: \t\t9.586776859504132%\n","\tloss: \t3.023946966443743\n","\tl1: \t\t136.0\n","\tp dist pair: \t1.2531322240829468\n","epoch: \t4\n","\twarm\n","\ttrain\n","\ttime: \t106.1436026096344\n","\tcross ent: \t1.9520758718252182\n","\tcluster: \t0.09627180888007085\n","\tseparation:\t0.06448553608109554\n","\tavg separation:\t0.25808994347850484\n","\taccu: \t\t39.125%\n","\tloss: \t2.0359344998995463\n","\tl1: \t\t136.0\n","\tp dist pair: \t1.1080601215362549\n","\ttest\n","\ttime: \t9.643869876861572\n","\tcross ent: \t2.9460534027644565\n","\tcluster: \t0.17774248336042678\n","\tseparation:\t0.05116246800337519\n","\tavg separation:\t0.23666521055357798\n","\taccu: \t\t9.090909090909092%\n","\tloss: \t3.0961544854300365\n","\tl1: \t\t136.0\n","\tp dist pair: \t1.1080601215362549\n","epoch: \t5\n","\tjoint\n","\ttrain\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"]},{"output_type":"stream","name":"stdout","text":["\ttime: \t297.55283093452454\n","\tcross ent: \t2.584975556532542\n","\tcluster: \t0.18570576440542935\n","\tseparation:\t0.09569033306712905\n","\tavg separation:\t0.27470331837733586\n","\taccu: \t\t12.1875%\n","\tloss: \t2.7378850251436235\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.6077750325202942\n","\ttest\n","\ttime: \t9.78029990196228\n","\tcross ent: \t2.6290575776781355\n","\tcluster: \t0.11358606496027537\n","\tseparation:\t0.056909658546958654\n","\tavg separation:\t0.2094301198210035\n","\taccu: \t\t9.421487603305785%\n","\tloss: \t2.7273737362452914\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.6077750325202942\n","\tabove 1.00%\n","epoch: \t6\n","\tjoint\n","\ttrain\n","\ttime: \t297.07749676704407\n","\tcross ent: \t2.39928457736969\n","\tcluster: \t0.08098789968838295\n","\tseparation:\t0.046417815672854586\n","\tavg separation:\t0.24526401087641717\n","\taccu: \t\t12.572916666666668%\n","\tloss: \t2.4723615487416586\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.632267415523529\n","\ttest\n","\ttime: \t9.718201160430908\n","\tcross ent: \t2.4956657205309187\n","\tcluster: \t0.07889885881117412\n","\tseparation:\t0.03965476926948343\n","\tavg separation:\t0.2910814370427813\n","\taccu: \t\t11.239669421487603%\n","\tloss: \t2.5676124777112688\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.632267415523529\n","epoch: \t7\n","\tjoint\n","\ttrain\n","\ttime: \t296.56654930114746\n","\tcross ent: \t2.2898154596487683\n","\tcluster: \t0.06981094240521392\n","\tseparation:\t0.04298691595904529\n","\tavg separation:\t0.31413239302734536\n","\taccu: \t\t14.406250000000002%\n","\tloss: \t2.354225335518519\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.6614797711372375\n","\ttest\n","\ttime: \t9.561964750289917\n","\tcross ent: \t2.3936988285609653\n","\tcluster: \t0.07995469123125076\n","\tseparation:\t0.04006934006299291\n","\tavg separation:\t0.32117738468306406\n","\taccu: \t\t10.578512396694215%\n","\tloss: \t2.4664571285247803\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.6614797711372375\n","epoch: \t8\n","\tjoint\n","\ttrain\n","\ttime: \t296.55123376846313\n","\tcross ent: \t2.2232689460118613\n","\tcluster: \t0.05555063029751182\n","\tseparation:\t0.0352244017024835\n","\tavg separation:\t0.3235224844266971\n","\taccu: \t\t15.354166666666666%\n","\tloss: \t2.276891589164734\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.7294382452964783\n","\ttest\n","\ttime: \t9.491151571273804\n","\tcross ent: \t2.450122833251953\n","\tcluster: \t0.07698023106370654\n","\tseparation:\t0.033867153738226206\n","\tavg separation:\t0.32640078025204794\n","\taccu: \t\t10.082644628099173%\n","\tloss: \t2.5209977626800537\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.7294382452964783\n","epoch: \t9\n","\tjoint\n","\ttrain\n","\ttime: \t296.82363986968994\n","\tcross ent: \t2.0096697092056273\n","\tcluster: \t0.03595196879468858\n","\tseparation:\t0.026250915974378584\n","\tavg separation:\t0.3286398100356261\n","\taccu: \t\t24.25%\n","\tloss: \t2.048331266641617\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.7335869669914246\n","\ttest\n","\ttime: \t9.578330993652344\n","\tcross ent: \t2.4919871262141635\n","\tcluster: \t0.06118628329464367\n","\tseparation:\t0.022041887310998782\n","\tavg separation:\t0.3391080115522657\n","\taccu: \t\t10.743801652892563%\n","\tloss: \t2.55117290360587\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.7335869669914246\n","epoch: \t10\n","\tjoint\n","\ttrain\n","\ttime: \t296.7669999599457\n","\tcross ent: \t1.8265226483345032\n","\tcluster: \t0.023902690798665086\n","\tseparation:\t0.018362985784187914\n","\tavg separation:\t0.3349721483886242\n","\taccu: \t\t33.08333333333333%\n","\tloss: \t1.8561757276455562\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.7331556677818298\n","\ttest\n","\ttime: \t9.514825820922852\n","\tcross ent: \t2.572666508810861\n","\tcluster: \t0.05434645472892693\n","\tseparation:\t0.016339329071342945\n","\tavg separation:\t0.33627464728696005\n","\taccu: \t\t9.917355371900827%\n","\tloss: \t2.6268366404942105\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.7331556677818298\n","\tabove 1.00%\n","\tpush\n","\tExecuting push ...\n","\tpush time: \t139.33614683151245\n","\ttest\n","\ttime: \t9.496376514434814\n","\tcross ent: \t2.827660288129534\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.75206611570248%\n","\tloss: \t2.875072956085205\n","\tl1: \t\t136.0\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","\tlast layer\n","iteration: \t0\n","\ttrain\n","\ttime: \t102.47812175750732\n","\tcross ent: \t1.8220748951037724\n","\tcluster: \t0.012009360191101829\n","\tseparation:\t0.007180670198674004\n","\tavg separation:\t0.3354780244330565\n","\taccu: \t\t35.64583333333333%\n","\tloss: \t1.843096163868904\n","\tl1: \t\t135.8301239013672\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.507791996002197\n","\tcross ent: \t2.8002261434282576\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.256198347107437%\n","\tloss: \t2.8476199763161794\n","\tl1: \t\t135.8301239013672\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t1\n","\ttrain\n","\ttime: \t105.2278208732605\n","\tcross ent: \t1.7875570317109426\n","\tcluster: \t0.012009360183340808\n","\tseparation:\t0.007180670229718089\n","\tavg separation:\t0.3354780269165834\n","\taccu: \t\t36.666666666666664%\n","\tloss: \t1.8085693498452504\n","\tl1: \t\t135.80877685546875\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.661000490188599\n","\tcross ent: \t2.805107797895159\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.090909090909092%\n","\tloss: \t2.85249924659729\n","\tl1: \t\t135.80877685546875\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t2\n","\ttrain\n","\ttime: \t105.04319953918457\n","\tcross ent: \t1.779670696457227\n","\tcluster: \t0.012009360167818764\n","\tseparation:\t0.007180670171510428\n","\tavg separation:\t0.33547802194952964\n","\taccu: \t\t36.895833333333336%\n","\tloss: \t1.8006833960612616\n","\tl1: \t\t135.8373565673828\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.59113621711731\n","\tcross ent: \t2.8097852298191617\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.256198347107437%\n","\tloss: \t2.8571795395442416\n","\tl1: \t\t135.8373565673828\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t3\n","\ttrain\n","\ttime: \t105.25805115699768\n","\tcross ent: \t1.7771359592676164\n","\tcluster: \t0.01200936019886285\n","\tseparation:\t0.007180670190912982\n","\tavg separation:\t0.3354780229429404\n","\taccu: \t\t36.864583333333336%\n","\tloss: \t1.7981521308422088\n","\tl1: \t\t135.87149047851562\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.649088382720947\n","\tcross ent: \t2.8127403599875316\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.090909090909092%\n","\tloss: \t2.860138245991298\n","\tl1: \t\t135.87149047851562\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t4\n","\ttrain\n","\ttime: \t104.96807599067688\n","\tcross ent: \t1.775765460729599\n","\tcluster: \t0.012009360167818764\n","\tseparation:\t0.007180670210315535\n","\tavg separation:\t0.33547802617152533\n","\taccu: \t\t36.895833333333336%\n","\tloss: \t1.796786218881607\n","\tl1: \t\t135.9153289794922\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.69179368019104\n","\tcross ent: \t2.8115266050611223\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.256198347107437%\n","\tloss: \t2.858929021017892\n","\tl1: \t\t135.9153289794922\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t5\n","\ttrain\n","\ttime: \t104.90102338790894\n","\tcross ent: \t1.7743475486834843\n","\tcluster: \t0.012009360144535701\n","\tseparation:\t0.007180670210315535\n","\tavg separation:\t0.33547802492976186\n","\taccu: \t\t36.864583333333336%\n","\tloss: \t1.7953724920749665\n","\tl1: \t\t135.957763671875\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.686586618423462\n","\tcross ent: \t2.8087665694100514\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.256198347107437%\n","\tloss: \t2.856173276901245\n","\tl1: \t\t135.957763671875\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t6\n","\ttrain\n","\ttime: \t104.90684294700623\n","\tcross ent: \t1.77328113814195\n","\tcluster: \t0.012009360214384894\n","\tseparation:\t0.0071806702335986\n","\tavg separation:\t0.33547802368799845\n","\taccu: \t\t36.97916666666667%\n","\tloss: \t1.7943103621403376\n","\tl1: \t\t135.9989471435547\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.585776567459106\n","\tcross ent: \t2.8101772580827986\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.256198347107437%\n","\tloss: \t2.8575882571084157\n","\tl1: \t\t135.9989471435547\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t7\n","\ttrain\n","\ttime: \t105.03027153015137\n","\tcross ent: \t1.7722075213988622\n","\tcluster: \t0.012009360183340808\n","\tseparation:\t0.007180670194793492\n","\tavg separation:\t0.3354780226945877\n","\taccu: \t\t36.916666666666664%\n","\tloss: \t1.7932410806417465\n","\tl1: \t\t136.04110717773438\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.518678188323975\n","\tcross ent: \t2.8093142168862477\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.090909090909092%\n","\tloss: \t2.856729745864868\n","\tl1: \t\t136.04110717773438\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t8\n","\ttrain\n","\ttime: \t104.78721356391907\n","\tcross ent: \t1.7711024244626363\n","\tcluster: \t0.012009360175579787\n","\tseparation:\t0.007180670225837579\n","\tavg separation:\t0.3354780226945877\n","\taccu: \t\t37.072916666666664%\n","\tloss: \t1.7921403537193934\n","\tl1: \t\t136.0789031982422\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.594935894012451\n","\tcross ent: \t2.811424664088658\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.090909090909092%\n","\tloss: \t2.8588442461831227\n","\tl1: \t\t136.0789031982422\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t9\n","\ttrain\n","\ttime: \t104.9952597618103\n","\tcross ent: \t1.7700854937235515\n","\tcluster: \t0.012009360175579787\n","\tseparation:\t0.0071806702335986\n","\tavg separation:\t0.3354780259231726\n","\taccu: \t\t36.97916666666667%\n","\tloss: \t1.7911272714535396\n","\tl1: \t\t136.11692810058594\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.56009578704834\n","\tcross ent: \t2.8098436083112444\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.090909090909092%\n","\tloss: \t2.8572672435215543\n","\tl1: \t\t136.11692810058594\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t10\n","\ttrain\n","\ttime: \t104.89736342430115\n","\tcross ent: \t1.7692729989687601\n","\tcluster: \t0.012009360183340808\n","\tseparation:\t0.007180670194793492\n","\tavg separation:\t0.33547802542646726\n","\taccu: \t\t37.0%\n","\tloss: \t1.7903187990188598\n","\tl1: \t\t136.14950561523438\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.625189542770386\n","\tcross ent: \t2.8097736835479736\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.256198347107437%\n","\tloss: \t2.85720089503697\n","\tl1: \t\t136.14950561523438\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t11\n","\ttrain\n","\ttime: \t104.92396450042725\n","\tcross ent: \t1.7682190428177516\n","\tcluster: \t0.012009360206623873\n","\tseparation:\t0.007180670214196046\n","\tavg separation:\t0.3354780229429404\n","\taccu: \t\t36.96875%\n","\tloss: \t1.7892685910065969\n","\tl1: \t\t136.1857147216797\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.589985132217407\n","\tcross ent: \t2.8131998607090543\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t8.925619834710744%\n","\tloss: \t2.860630886895316\n","\tl1: \t\t136.1857147216797\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t12\n","\ttrain\n","\ttime: \t104.74011969566345\n","\tcross ent: \t1.7674698760112126\n","\tcluster: \t0.012009360206623873\n","\tseparation:\t0.007180670210315535\n","\tavg separation:\t0.3354780239363511\n","\taccu: \t\t37.0%\n","\tloss: \t1.7885231961806616\n","\tl1: \t\t136.2192840576172\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.500086545944214\n","\tcross ent: \t2.814713273729597\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.090909090909092%\n","\tloss: \t2.862147876194545\n","\tl1: \t\t136.2192840576172\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t13\n","\ttrain\n","\ttime: \t104.92188048362732\n","\tcross ent: \t1.7665408958991369\n","\tcluster: \t0.012009360222145915\n","\tseparation:\t0.007180670214196046\n","\tavg separation:\t0.33547802542646726\n","\taccu: \t\t37.041666666666664%\n","\tloss: \t1.7875975747903188\n","\tl1: \t\t136.2549591064453\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.67000699043274\n","\tcross ent: \t2.8144138881138394\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.090909090909092%\n","\tloss: \t2.861852305276053\n","\tl1: \t\t136.2549591064453\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t14\n","\ttrain\n","\ttime: \t104.74451041221619\n","\tcross ent: \t1.7656929592291515\n","\tcluster: \t0.012009360222145915\n","\tseparation:\t0.007180670194793492\n","\tavg separation:\t0.3354780226945877\n","\taccu: \t\t37.02083333333333%\n","\tloss: \t1.786753319700559\n","\tl1: \t\t136.28729248046875\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.546768426895142\n","\tcross ent: \t2.813147851399013\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t9.090909090909092%\n","\tloss: \t2.8605898448399136\n","\tl1: \t\t136.28729248046875\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t15\n","\ttrain\n","\ttime: \t104.82999467849731\n","\tcross ent: \t1.764857318997383\n","\tcluster: \t0.012009360175579787\n","\tseparation:\t0.007180670221957067\n","\tavg separation:\t0.3354780221978823\n","\taccu: \t\t37.083333333333336%\n","\tloss: \t1.7859213948249817\n","\tl1: \t\t136.3208465576172\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.75321340560913\n","\tcross ent: \t2.8125617844717845\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t8.760330578512397%\n","\tloss: \t2.8600073541913713\n","\tl1: \t\t136.3208465576172\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t16\n","\ttrain\n","\ttime: \t104.93872237205505\n","\tcross ent: \t1.764145122965177\n","\tcluster: \t0.012009360191101829\n","\tseparation:\t0.007180670218076557\n","\tavg separation:\t0.33547802170117696\n","\taccu: \t\t36.97916666666667%\n","\tloss: \t1.7852127740780512\n","\tl1: \t\t136.35009765625\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.541780233383179\n","\tcross ent: \t2.8134400503976003\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t8.925619834710744%\n","\tloss: \t2.860888719558716\n","\tl1: \t\t136.35009765625\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t17\n","\ttrain\n","\ttime: \t104.77764010429382\n","\tcross ent: \t1.763378026088079\n","\tcluster: \t0.012009360183340808\n","\tseparation:\t0.007180670214196046\n","\tavg separation:\t0.33547802368799845\n","\taccu: \t\t37.041666666666664%\n","\tloss: \t1.7844489326079687\n","\tl1: \t\t136.3807373046875\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.680018424987793\n","\tcross ent: \t2.814004591533116\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t8.925619834710744%\n","\tloss: \t2.8614565985543385\n","\tl1: \t\t136.3807373046875\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t18\n","\ttrain\n","\ttime: \t104.8027868270874\n","\tcross ent: \t1.7626093159119287\n","\tcluster: \t0.012009360222145915\n","\tseparation:\t0.007180670202554514\n","\tavg separation:\t0.3354780239363511\n","\taccu: \t\t37.052083333333336%\n","\tloss: \t1.7836834639310837\n","\tl1: \t\t136.41372680664062\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.523037195205688\n","\tcross ent: \t2.8139633451189314\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t8.925619834710744%\n","\tloss: \t2.8614189284188405\n","\tl1: \t\t136.41372680664062\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","iteration: \t19\n","\ttrain\n","\ttime: \t104.77630591392517\n","\tcross ent: \t1.761812123656273\n","\tcluster: \t0.012009360160057743\n","\tseparation:\t0.007180670214196046\n","\tavg separation:\t0.3354780234396458\n","\taccu: \t\t36.97916666666667%\n","\tloss: \t1.7828898936510087\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7203282117843628\n","\ttest\n","\ttime: \t9.7580726146698\n","\tcross ent: \t2.8155247824532643\n","\tcluster: \t0.044984261106167524\n","\tseparation:\t0.007184687769040465\n","\tavg separation:\t0.3270100163561957\n","\taccu: \t\t8.925619834710744%\n","\tloss: \t2.862983465194702\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7203282117843628\n","\tabove 1.00%\n","epoch: \t11\n","\tjoint\n","\ttrain\n","\ttime: \t296.80587673187256\n","\tcross ent: \t1.670320243636767\n","\tcluster: \t0.015038421369778614\n","\tseparation:\t0.009993691633765896\n","\tavg separation:\t0.3313442476093769\n","\taccu: \t\t39.5625%\n","\tloss: \t1.6935974647601446\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7195733189582825\n","\ttest\n","\ttime: \t9.561630725860596\n","\tcross ent: \t2.6191603796822682\n","\tcluster: \t0.04633945891899722\n","\tseparation:\t0.010781086316066129\n","\tavg separation:\t0.31589662602969576\n","\taccu: \t\t10.743801652892563%\n","\tloss: \t2.667415584836687\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7195733189582825\n","epoch: \t12\n","\tjoint\n","\ttrain\n","\ttime: \t296.82683181762695\n","\tcross ent: \t1.4629140804211298\n","\tcluster: \t0.011989352161375184\n","\tseparation:\t0.00911899427883327\n","\tavg separation:\t0.32853687430421513\n","\taccu: \t\t49.47916666666667%\n","\tloss: \t1.4838220179080963\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7158307433128357\n","\ttest\n","\ttime: \t9.554290771484375\n","\tcross ent: \t2.6799971035548618\n","\tcluster: \t0.04182984387235982\n","\tseparation:\t0.008627133909612894\n","\tavg separation:\t0.3070585961852755\n","\taccu: \t\t10.413223140495868%\n","\tloss: \t2.7248169354030063\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7158307433128357\n","epoch: \t13\n","\tjoint\n","\ttrain\n","\ttime: \t296.89239478111267\n","\tcross ent: \t1.3045907139778137\n","\tcluster: \t0.009717160238263507\n","\tseparation:\t0.008449824806302787\n","\tavg separation:\t0.3260428585112095\n","\taccu: \t\t55.833333333333336%\n","\tloss: \t1.3237344364325205\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7152971625328064\n","\ttest\n","\ttime: \t9.550551652908325\n","\tcross ent: \t2.6744931765965054\n","\tcluster: \t0.04439360462129116\n","\tseparation:\t0.008861803250121219\n","\tavg separation:\t0.3092191517353058\n","\taccu: \t\t9.256198347107437%\n","\tloss: \t2.7213451862335205\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7152971625328064\n","epoch: \t14\n","\tjoint\n","\ttrain\n","\ttime: \t297.0810685157776\n","\tcross ent: \t1.1555048197507858\n","\tcluster: \t0.008463321668871988\n","\tseparation:\t0.008540365130951006\n","\tavg separation:\t0.32506979008515674\n","\taccu: \t\t62.68749999999999%\n","\tloss: \t1.173638233045737\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7145553231239319\n","\ttest\n","\ttime: \t9.539233207702637\n","\tcross ent: \t2.7331368582589284\n","\tcluster: \t0.04469100891479424\n","\tseparation:\t0.007960597319262368\n","\tavg separation:\t0.3159256598779133\n","\taccu: \t\t9.256198347107437%\n","\tloss: \t2.780298948287964\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7145553231239319\n","epoch: \t15\n","\tjoint\n","\ttrain\n","\ttime: \t297.1232240200043\n","\tcross ent: \t1.1082620506485303\n","\tcluster: \t0.007834918637915205\n","\tseparation:\t0.008170460214993605\n","\tavg separation:\t0.32589785431822144\n","\taccu: \t\t64.48958333333333%\n","\tloss: \t1.125922337671121\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7139574289321899\n","\ttest\n","\ttime: \t9.573617219924927\n","\tcross ent: \t2.764100040708269\n","\tcluster: \t0.04488552574600492\n","\tseparation:\t0.0077437482375119415\n","\tavg separation:\t0.31921279430389404\n","\taccu: \t\t9.75206611570248%\n","\tloss: \t2.8114350863865445\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7139574289321899\n","\tabove 1.00%\n","epoch: \t16\n","\tjoint\n","\ttrain\n","\ttime: \t296.87964963912964\n","\tcross ent: \t1.0813041796286902\n","\tcluster: \t0.007450418597242484\n","\tseparation:\t0.007958238253680368\n","\tavg separation:\t0.32550218924880026\n","\taccu: \t\t65.72916666666667%\n","\tloss: \t1.0986738393704096\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7134656310081482\n","\ttest\n","\ttime: \t9.506653547286987\n","\tcross ent: \t2.7604030881609236\n","\tcluster: \t0.04359405407948153\n","\tseparation:\t0.007318921520241669\n","\tavg separation:\t0.31553572842053007\n","\taccu: \t\t9.917355371900827%\n","\tloss: \t2.806738921574184\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7134656310081482\n","epoch: \t17\n","\tjoint\n","\ttrain\n","\ttime: \t296.8137676715851\n","\tcross ent: \t1.057976160943508\n","\tcluster: \t0.007124764220012973\n","\tseparation:\t0.007780502779254069\n","\tavg separation:\t0.3251476009686788\n","\taccu: \t\t66.48958333333333%\n","\tloss: \t1.0750995188951493\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7129034996032715\n","\ttest\n","\ttime: \t9.529861450195312\n","\tcross ent: \t2.7814882823399136\n","\tcluster: \t0.04388145317456552\n","\tseparation:\t0.0073384638609630725\n","\tavg separation:\t0.31652154667036875\n","\taccu: \t\t9.75206611570248%\n","\tloss: \t2.828052418572562\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7129034996032715\n","epoch: \t18\n","\tjoint\n","\ttrain\n","\ttime: \t297.0838282108307\n","\tcross ent: \t1.037306796014309\n","\tcluster: \t0.006859377321476737\n","\tseparation:\t0.00762789340224117\n","\tavg separation:\t0.3246724893649419\n","\taccu: \t\t67.30208333333333%\n","\tloss: \t1.0542300591866176\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7123647332191467\n","\ttest\n","\ttime: \t9.427942752838135\n","\tcross ent: \t2.7946480001722063\n","\tcluster: \t0.04278758394398859\n","\tseparation:\t0.006984438148460218\n","\tavg separation:\t0.3149702932153429\n","\taccu: \t\t9.917355371900827%\n","\tloss: \t2.840365409851074\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7123647332191467\n","epoch: \t19\n","\tjoint\n","\ttrain\n","\ttime: \t296.90157079696655\n","\tcross ent: \t1.0157421658436456\n","\tcluster: \t0.006608381370703379\n","\tseparation:\t0.007534730546952536\n","\tavg separation:\t0.3241171528895696\n","\taccu: \t\t68.375%\n","\tloss: \t1.032472095390161\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.712402880191803\n","\ttest\n","\ttime: \t9.524417161941528\n","\tcross ent: \t2.7874832493918285\n","\tcluster: \t0.043040095163243156\n","\tseparation:\t0.0070463014500481745\n","\tavg separation:\t0.31511218207223074\n","\taccu: \t\t9.917355371900827%\n","\tloss: \t2.8333976609366283\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.712402880191803\n","epoch: \t20\n","\tjoint\n","\ttrain\n","\ttime: \t297.0365128517151\n","\tcross ent: \t1.0126018211245538\n","\tcluster: \t0.0065991315253389375\n","\tseparation:\t0.007545643773240348\n","\tavg separation:\t0.3243301418920358\n","\taccu: \t\t68.39583333333333%\n","\tloss: \t1.029323480029901\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7122675776481628\n","\ttest\n","\ttime: \t9.748974323272705\n","\tcross ent: \t2.789163385118757\n","\tcluster: \t0.04296490164207561\n","\tseparation:\t0.006989767708416496\n","\tavg separation:\t0.3151129846061979\n","\taccu: \t\t10.082644628099173%\n","\tloss: \t2.835022211074829\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7122675776481628\n","\tabove 1.00%\n","\tpush\n","\tExecuting push ...\n","\tpush time: \t18.33483910560608\n","\ttest\n","\ttime: \t9.601691961288452\n","\tcross ent: \t2.8496785163879395\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t10.24793388429752%\n","\tloss: \t2.8954512732369557\n","\tl1: \t\t136.441650390625\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","\tlast layer\n","iteration: \t0\n","\ttrain\n","\ttime: \t105.47025871276855\n","\tcross ent: \t1.055944057305654\n","\tcluster: \t0.006302123179193586\n","\tseparation:\t0.006768952337248872\n","\tavg separation:\t0.32452687323093415\n","\taccu: \t\t66.14583333333334%\n","\tloss: \t1.0724916925032935\n","\tl1: \t\t136.49481201171875\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.736044645309448\n","\tcross ent: \t2.8432528972625732\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t10.909090909090908%\n","\tloss: \t2.889028991971697\n","\tl1: \t\t136.49481201171875\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t1\n","\ttrain\n","\ttime: \t105.00165748596191\n","\tcross ent: \t1.0477649932106337\n","\tcluster: \t0.006302123161731288\n","\tseparation:\t0.006768952310085297\n","\tavg separation:\t0.32452687496940297\n","\taccu: \t\t66.55208333333333%\n","\tloss: \t1.0643167302012444\n","\tl1: \t\t136.56533813476562\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.672657251358032\n","\tcross ent: \t2.844780342919486\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.239669421487603%\n","\tloss: \t2.890560967581613\n","\tl1: \t\t136.56533813476562\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t2\n","\ttrain\n","\ttime: \t105.04206871986389\n","\tcross ent: \t1.046288087964058\n","\tcluster: \t0.006302123171432565\n","\tseparation:\t0.006768952321726829\n","\tavg separation:\t0.3245268742243449\n","\taccu: \t\t66.8125%\n","\tloss: \t1.0628447214762369\n","\tl1: \t\t136.64256286621094\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.684611320495605\n","\tcross ent: \t2.846579006740025\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.074380165289256%\n","\tloss: \t2.892365115029471\n","\tl1: \t\t136.64256286621094\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t3\n","\ttrain\n","\ttime: \t104.92874073982239\n","\tcross ent: \t1.0456429198384285\n","\tcluster: \t0.006302123155910522\n","\tseparation:\t0.006768952282921722\n","\tavg separation:\t0.3245268762111664\n","\taccu: \t\t66.78125%\n","\tloss: \t1.0622051060199738\n","\tl1: \t\t136.7235107421875\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.654506206512451\n","\tcross ent: \t2.8463123185294017\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.074380165289256%\n","\tloss: \t2.892103910446167\n","\tl1: \t\t136.7235107421875\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t4\n","\ttrain\n","\ttime: \t105.21529269218445\n","\tcross ent: \t1.044933723906676\n","\tcluster: \t0.0063021231753130754\n","\tseparation:\t0.0067689522984437644\n","\tavg separation:\t0.3245268742243449\n","\taccu: \t\t66.78125%\n","\tloss: \t1.061501834789912\n","\tl1: \t\t136.8050537109375\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.790586709976196\n","\tcross ent: \t2.849797010421753\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.239669421487603%\n","\tloss: \t2.8955943243844167\n","\tl1: \t\t136.8050537109375\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t5\n","\ttrain\n","\ttime: \t105.16869902610779\n","\tcross ent: \t1.0443284009893736\n","\tcluster: \t0.00630212316949231\n","\tseparation:\t0.00676895232560734\n","\tavg separation:\t0.3245268762111664\n","\taccu: \t\t66.85416666666667%\n","\tloss: \t1.0609022825956345\n","\tl1: \t\t136.8895721435547\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.682677268981934\n","\tcross ent: \t2.8516249656677246\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.239669421487603%\n","\tloss: \t2.897428001676287\n","\tl1: \t\t136.8895721435547\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t6\n","\ttrain\n","\ttime: \t105.15500020980835\n","\tcross ent: \t1.0435865566134452\n","\tcluster: \t0.006302123167552054\n","\tseparation:\t0.006768952313965807\n","\tavg separation:\t0.32452687447269757\n","\taccu: \t\t66.82291666666667%\n","\tloss: \t1.0601661324501037\n","\tl1: \t\t136.97593688964844\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.730172157287598\n","\tcross ent: \t2.8529089178357805\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.40495867768595%\n","\tloss: \t2.8987179143088206\n","\tl1: \t\t136.97593688964844\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t7\n","\ttrain\n","\ttime: \t105.0409836769104\n","\tcross ent: \t1.0429790238539378\n","\tcluster: \t0.006302123188894863\n","\tseparation:\t0.006768952321726829\n","\tavg separation:\t0.3245268762111664\n","\taccu: \t\t66.88541666666666%\n","\tloss: \t1.0595646585027376\n","\tl1: \t\t137.06394958496094\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.567231178283691\n","\tcross ent: \t2.854092700140817\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.239669421487603%\n","\tloss: \t2.8999078954969133\n","\tl1: \t\t137.06394958496094\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t8\n","\ttrain\n","\ttime: \t105.09797620773315\n","\tcross ent: \t1.0423336893320083\n","\tcluster: \t0.0063021231753130754\n","\tseparation:\t0.00676895232948785\n","\tavg separation:\t0.32452687323093415\n","\taccu: \t\t66.8125%\n","\tloss: \t1.0589252044757207\n","\tl1: \t\t137.15267944335938\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.602889776229858\n","\tcross ent: \t2.85397584097726\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.074380165289256%\n","\tloss: \t2.8997969967978343\n","\tl1: \t\t137.15267944335938\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t9\n","\ttrain\n","\ttime: \t105.1422770023346\n","\tcross ent: \t1.0416960909962654\n","\tcluster: \t0.0063021231830740964\n","\tseparation:\t0.006768952302324275\n","\tavg separation:\t0.32452687472105024\n","\taccu: \t\t66.85416666666667%\n","\tloss: \t1.058294094602267\n","\tl1: \t\t137.24241638183594\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.648211240768433\n","\tcross ent: \t2.856184244155884\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.239669421487603%\n","\tloss: \t2.9020115988595143\n","\tl1: \t\t137.24241638183594\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t10\n","\ttrain\n","\ttime: \t105.09040784835815\n","\tcross ent: \t1.0409479856491088\n","\tcluster: \t0.0063021231830740964\n","\tseparation:\t0.0067689522984437644\n","\tavg separation:\t0.32452687149246534\n","\taccu: \t\t66.91666666666667%\n","\tloss: \t1.0575521474083265\n","\tl1: \t\t137.33599853515625\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.67287015914917\n","\tcross ent: \t2.8560119356427873\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.40495867768595%\n","\tloss: \t2.901845727648054\n","\tl1: \t\t137.33599853515625\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t11\n","\ttrain\n","\ttime: \t105.04981708526611\n","\tcross ent: \t1.0404204552372296\n","\tcluster: \t0.006302123155910522\n","\tseparation:\t0.006768952317846318\n","\tavg separation:\t0.324526875714461\n","\taccu: \t\t66.86458333333334%\n","\tloss: \t1.0570309768120447\n","\tl1: \t\t137.43023681640625\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.553244352340698\n","\tcross ent: \t2.8573781762804304\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.239669421487603%\n","\tloss: \t2.9032186440059116\n","\tl1: \t\t137.43023681640625\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t12\n","\ttrain\n","\ttime: \t105.18867325782776\n","\tcross ent: \t1.0396853327751159\n","\tcluster: \t0.006302123190835118\n","\tseparation:\t0.00676895232560734\n","\tavg separation:\t0.3245268729825815\n","\taccu: \t\t66.85416666666667%\n","\tloss: \t1.0563026681542396\n","\tl1: \t\t137.52288818359375\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.64082956314087\n","\tcross ent: \t2.8605383464268277\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.239669421487603%\n","\tloss: \t2.9063850130353654\n","\tl1: \t\t137.52288818359375\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t13\n","\ttrain\n","\ttime: \t104.90879917144775\n","\tcross ent: \t1.0389816413323085\n","\tcluster: \t0.006302123159791033\n","\tseparation:\t0.0067689522984437644\n","\tavg separation:\t0.3245268754661083\n","\taccu: \t\t66.88541666666666%\n","\tloss: \t1.0556050782402358\n","\tl1: \t\t137.6143798828125\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.546700716018677\n","\tcross ent: \t2.859470878328596\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.239669421487603%\n","\tloss: \t2.9053237438201904\n","\tl1: \t\t137.6143798828125\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t14\n","\ttrain\n","\ttime: \t105.12552881240845\n","\tcross ent: \t1.0383881598711013\n","\tcluster: \t0.006302123171432565\n","\tseparation:\t0.006768952310085297\n","\tavg separation:\t0.32452687273422876\n","\taccu: \t\t66.9375%\n","\tloss: \t1.0550178016225498\n","\tl1: \t\t137.71127319335938\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.649034023284912\n","\tcross ent: \t2.8630261761801585\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.40495867768595%\n","\tloss: \t2.9088857173919678\n","\tl1: \t\t137.71127319335938\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t15\n","\ttrain\n","\ttime: \t104.97240805625916\n","\tcross ent: \t1.0377877801656723\n","\tcluster: \t0.006302123163671543\n","\tseparation:\t0.006768952317846318\n","\tavg separation:\t0.32452687447269757\n","\taccu: \t\t66.86458333333334%\n","\tloss: \t1.0544241070747375\n","\tl1: \t\t137.80609130859375\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.637702465057373\n","\tcross ent: \t2.863945586340768\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.074380165289256%\n","\tloss: \t2.909811564854213\n","\tl1: \t\t137.80609130859375\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t16\n","\ttrain\n","\ttime: \t105.16277885437012\n","\tcross ent: \t1.0370571240782738\n","\tcluster: \t0.006302123167552054\n","\tseparation:\t0.006768952310085297\n","\tavg separation:\t0.3245268742243449\n","\taccu: \t\t66.8125%\n","\tloss: \t1.053699823220571\n","\tl1: \t\t137.90573120117188\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.793045282363892\n","\tcross ent: \t2.866550922393799\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t10.909090909090908%\n","\tloss: \t2.9124238150460378\n","\tl1: \t\t137.90573120117188\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t17\n","\ttrain\n","\ttime: \t105.08823585510254\n","\tcross ent: \t1.0363555729389191\n","\tcluster: \t0.006302123159791033\n","\tseparation:\t0.006768952313965807\n","\tavg separation:\t0.32452687447269757\n","\taccu: \t\t66.89583333333333%\n","\tloss: \t1.0530049428343773\n","\tl1: \t\t138.0025177001953\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.738081216812134\n","\tcross ent: \t2.8677572863442555\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.074380165289256%\n","\tloss: \t2.9136366162981306\n","\tl1: \t\t138.0025177001953\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t18\n","\ttrain\n","\ttime: \t105.13787198066711\n","\tcross ent: \t1.035714220503966\n","\tcluster: \t0.006302123171432565\n","\tseparation:\t0.00676895232560734\n","\tavg separation:\t0.3245268722375234\n","\taccu: \t\t66.85416666666667%\n","\tloss: \t1.0523705025513967\n","\tl1: \t\t138.0997314453125\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.83823537826538\n","\tcross ent: \t2.8704092502593994\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.074380165289256%\n","\tloss: \t2.916295255933489\n","\tl1: \t\t138.0997314453125\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n","iteration: \t19\n","\ttrain\n","\ttime: \t105.23395848274231\n","\tcross ent: \t1.0350236838062605\n","\tcluster: \t0.006302123167552054\n","\tseparation:\t0.006768952345009893\n","\tavg separation:\t0.32452687323093415\n","\taccu: \t\t66.84375%\n","\tloss: \t1.0516865541537603\n","\tl1: \t\t138.19334411621094\n","\tp dist pair: \t0.7126322984695435\n","\ttest\n","\ttime: \t9.618185043334961\n","\tcross ent: \t2.8711069311414446\n","\tcluster: \t0.04277884268334934\n","\tseparation:\t0.006204844718532903\n","\tavg separation:\t0.3159346410206386\n","\taccu: \t\t11.239669421487603%\n","\tloss: \t2.916999135698591\n","\tl1: \t\t138.19334411621094\n","\tp dist pair: \t0.7126322984695435\n","\tabove 1.00%\n"]}],"source":["1## Ok\n","\n","epoches_accuracy_saved = [i for i in range(1000) if i % 10 == 0] # show the epches to the accuracy logs be saved\n","# book keeping namings and code\n","#from settings import base_architecture, img_size, prototype_shape, num_classes, n_fold, \\\n","#                     prototype_activation_function, add_on_layers_type, experiment_run\n","\n","base_architecture_type = re.match('^[a-z]*', base_architecture).group(0)\n","\n","model_dir = './saved_models'+'_'+'F'+str(n_fold)+'_'+str(num_classes)+'/' + base_architecture + '/' + experiment_run + '/'\n","makedir(model_dir)\n","\n","# shutil.copy(src=os.path.join(os.getcwd(), __file__), dst=model_dir)  # I think this line should not be implemented (__file__ doesn't exist!)\n","shutil.copy(src=os.path.join(os.getcwd(), 'settings.py'), dst=model_dir)\n","shutil.copy(src=os.path.join(os.getcwd(), base_architecture_type + '_features.py'), dst=model_dir)\n","shutil.copy(src=os.path.join(os.getcwd(), 'model.py'), dst=model_dir)\n","shutil.copy(src=os.path.join(os.getcwd(), 'train_and_test.py'), dst=model_dir)\n","\n","log, logclose = create_logger(log_filename=os.path.join(model_dir, 'train.log'))\n","img_dir = os.path.join(model_dir, 'img')\n","makedir(img_dir) # Directory for saving prototypes\n","weight_matrix_filename = 'outputL_weights'\n","prototype_img_filename_prefix = 'prototype-img'\n","prototype_self_act_filename_prefix = 'prototype-self-act'\n","proto_bound_boxes_filename_prefix = 'bb'\n","\n","# load the data\n","from settings import train_dir, test_dir, train_push_dir, \\\n","                     train_batch_size, test_batch_size, train_push_batch_size\n","\n","# test_dir = train_push_dir\n","normalize = transforms.Normalize(mean=mean,\n","                                 std=std)\n","\n","# all datasets\n","# train set\n","\n","train_dataset = datasets.ImageFolder(\n","    train_dir,\n","    transforms.Compose([\n","        transforms.Resize(size=(img_size, img_size)),\n","        transforms.ToTensor(),\n","        normalize,\n","    ]))\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset , batch_size=train_batch_size, shuffle=True,\n","    num_workers=4, pin_memory=False)\n","\n","# push set\n","train_push_dataset = datasets.ImageFolder(\n","    train_push_dir,\n","    transforms.Compose([\n","        transforms.Resize(size=(img_size, img_size)),\n","        transforms.ToTensor(),\n","    ]))\n","train_push_loader = torch.utils.data.DataLoader(\n","    train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n","    num_workers=4, pin_memory=False)\n","\n","# test set\n","test_dataset = datasets.ImageFolder(\n","    test_dir,\n","    transforms.Compose([\n","        transforms.Resize(size=(img_size, img_size)),\n","        transforms.ToTensor(),\n","        normalize,\n","    ]))\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=test_batch_size, shuffle=False,\n","    num_workers=4, pin_memory=False)\n","\n","\n","\n","\n","\n","# we should look into distributed sampler more carefully at torch.utils.data.distributed.DistributedSampler(train_dataset)\n","log('training set size: {0}'.format(len(train_loader.dataset)))\n","log('push set size: {0}'.format(len(train_push_loader.dataset)))\n","log('test set size: {0}'.format(len(test_loader.dataset)))\n","log('batch size: {0}'.format(train_batch_size))\n","\n","# construct the model\n","ppnet = model.construct_PPNet(base_architecture=base_architecture,\n","                              pretrained=True, img_size=img_size,\n","                              prototype_shape=prototype_shape,\n","                              num_classes=num_classes,\n","                              prototype_activation_function=prototype_activation_function,\n","                              add_on_layers_type=add_on_layers_type)\n","#if prototype_activation_function == 'linear':\n","#    ppnet.set_last_layer_incorrect_connection(incorrect_strength=0)\n","\n","ppnet = ppnet.cuda()\n","ppnet_multi = torch.nn.DataParallel(ppnet)  # ??\n","class_specific = True\n","\n","# define optimizer\n","from settings import joint_optimizer_lrs, joint_lr_step_size\n","joint_optimizer_specs = \\\n","[{'params': ppnet.features.parameters(), 'lr': joint_optimizer_lrs['features'], 'weight_decay': 1e-3}, # bias are now also being regularized\n"," {'params': ppnet.add_on_layers.parameters(), 'lr': joint_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n"," {'params': ppnet.prototype_vectors, 'lr': joint_optimizer_lrs['prototype_vectors']},\n","]\n","joint_optimizer = torch.optim.Adam(joint_optimizer_specs)\n","joint_lr_scheduler = torch.optim.lr_scheduler.StepLR(joint_optimizer, step_size=joint_lr_step_size, gamma=0.1)\n","\n","from settings import warm_optimizer_lrs\n","warm_optimizer_specs = \\\n","[{'params': ppnet.add_on_layers.parameters(), 'lr': warm_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n"," {'params': ppnet.prototype_vectors, 'lr': warm_optimizer_lrs['prototype_vectors']},\n","]\n","warm_optimizer = torch.optim.Adam(warm_optimizer_specs)\n","\n","from settings import last_layer_optimizer_lr\n","last_layer_optimizer_specs = [{'params': ppnet.last_layer.parameters(), 'lr': last_layer_optimizer_lr}]\n","last_layer_optimizer = torch.optim.Adam(last_layer_optimizer_specs)\n","\n","# weighting of different training losses\n","from settings import coefs\n","\n","# number of training epochs, number of warm epochs, push start epoch, push epochs\n","from settings import num_train_epochs, num_warm_epochs, push_start, push_epochs\n","\n","test_epochs = [i for i in range(num_train_epochs) if i % 5 == 0]\n","# train the model\n","log('start training')\n","import copy\n","\n","train_acc_log = []\n","test_acc_log = []\n","train_loss_log = []\n","test_loss_log = []\n","\n","for epoch in range(num_train_epochs):\n","    log('epoch: \\t{0}'.format(epoch))\n","\n","    if epoch < num_warm_epochs:\n","        tnt.warm_only(model=ppnet_multi, log=log)\n","        acc_train, loss_train = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=warm_optimizer,\n","                      class_specific=class_specific, coefs=coefs, log=log)\n","    else:\n","        tnt.joint(model=ppnet_multi, log=log)\n","        joint_lr_scheduler.step()\n","        acc_train, loss_train = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=joint_optimizer,\n","                      class_specific=class_specific, coefs=coefs, log=log)\n","    accu, loss = tnt.test(model=ppnet_multi, dataloader=test_loader,\n","                    class_specific=class_specific, log=log)\n","    if epoch >= 5 and epoch in test_epochs:\n","      save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'nopush', accu=accu,\n","                                  target_accu = target_test_accu, log=log)\n","\n","    train_acc_log.append(acc_train)\n","    test_acc_log.append(accu)\n","    train_loss_log.append(loss_train)\n","    test_loss_log.append(loss)\n","\n","    if epoch >= push_start and epoch in push_epochs:\n","        push.push_prototypes(\n","            train_push_loader, # pytorch dataloader (must be unnormalized in [0,1])\n","            prototype_network_parallel=ppnet_multi, # pytorch network with prototype_vectors\n","            class_specific=class_specific,\n","            preprocess_input_function=preprocess_input_function, # normalize if needed\n","            prototype_layer_stride=1,\n","            root_dir_for_saving_prototypes=img_dir, # if not None, prototypes will be saved here\n","            epoch_number=epoch, # if not provided, prototypes saved previously will be overwritten\n","            prototype_img_filename_prefix=prototype_img_filename_prefix,\n","            prototype_self_act_filename_prefix=prototype_self_act_filename_prefix,\n","            proto_bound_boxes_filename_prefix=proto_bound_boxes_filename_prefix,\n","            save_prototype_class_identity=True,\n","            log=log)\n","        accu, loss = tnt.test(model=ppnet_multi, dataloader=test_loader,\n","                        class_specific=class_specific, log=log)\n","        save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'push', accu=accu,\n","                                    target_accu = target_test_accu, log=log)\n","\n","        if prototype_activation_function != 'linear':\n","            tnt.last_only(model=ppnet_multi, log=log)\n","            for i in range(20):\n","                log('iteration: \\t{0}'.format(i))\n","                acc_train, loss_train = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=last_layer_optimizer,\n","                              class_specific=class_specific, coefs=coefs, log=log)\n","                accu, loss = tnt.test(model=ppnet_multi, dataloader=test_loader,\n","                                class_specific=class_specific, log=log)\n","                save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + '_' + str(i) + 'push', accu=accu,\n","                                            target_accu = target_test_accu, log=log)\n","\n","                train_acc_log.append(acc_train)\n","                test_acc_log.append(accu)\n","                train_loss_log.append(loss_train)\n","                test_loss_log.append(loss)\n","    if epoch >= 10 and epoch in epoches_accuracy_saved:\n","       #torch.save(train_acc_log,test_acc_log,train_loss_log,test_loss_log, 'Train_test_loss_log_f_'+str(n_fold)+'_c_'+str(num_classes)+'.pt')\n","       torch.save(train_acc_log, 'Train_log_f_'+str(n_fold)+'_c_'+str(num_classes)+'.pt')\n","       torch.save(test_acc_log, 'Test_log_f_'+str(n_fold)+'_c_'+str(num_classes)+'.pt')\n","       torch.save(train_loss_log, 'Train_log_loss_f_'+str(n_fold)+'_c_'+str(num_classes)+'.pt')\n","       torch.save(test_loss_log, 'Test_log_loss_f_'+str(n_fold)+'_c_'+str(num_classes)+'.pt')\n","\n","logclose()"]},{"cell_type":"markdown","metadata":{"id":"rHWt7kQc2dEy"},"source":["# Compute the Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6231,"status":"ok","timestamp":1709339873776,"user":{"displayName":"MJ Lab","userId":"12526651074320095318"},"user_tz":-210},"id":"D4YitrUX2bYt","outputId":"dbb38b1f-3e39-4146-8e30-2df20c41a4bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[H\u001b[2Jtraining set size: 9600\n","push set size: 995\n","test set size: 605\n","batch size: 1\n"]}],"source":["# Set the trained model name here\n","model_name = '30nopush0.2066.pth'   #\"30nopush0.1124.pth\"\n","\n","model_dir = './saved_models'+'_'+'F'+str(n_fold)+'_'+str(num_classes)+'/' + base_architecture + '/' + experiment_run + '/' # _Fold1_8\n","PATH = model_dir + model_name\n","test_batch_size = 1  # Note! When we iteratively load and process batches of data, large batch sizes may cause the CUDA to be out of memory\n","train_batch_size = 1\n","train_push_batch_size = 1\n","# book keeping namings and code\n","#from settings import base_architecture, img_size, prototype_shape, num_classes, \\\n","#                     prototype_activation_function, add_on_layers_type, experiment_run\n","\n","base_architecture_type = re.match('^[a-z]*', base_architecture).group(0)\n","%clear model, ppnet\n","# construct the model\n","ppnet = model.construct_PPNet(base_architecture=base_architecture,\n","                              pretrained=True, img_size=img_size,\n","                              prototype_shape=prototype_shape,\n","                              num_classes=num_classes,\n","                              prototype_activation_function=prototype_activation_function,\n","                              add_on_layers_type=add_on_layers_type)\n","#if prototype_activation_function == 'linear':\n","#    ppnet.set_last_layer_incorrect_connection(incorrect_strength=0)\n","ppnet = ppnet.cuda()\n","ppnet_multi = torch.nn.DataParallel(ppnet)  # ??\n","class_specific = True\n","\n","model = ppnet #_multi\n","model = torch.load(PATH)\n","#model = model.cuda()\n","\n","shutil.copy(src=os.path.join(os.getcwd(), 'settings.py'), dst=model_dir)\n","shutil.copy(src=os.path.join(os.getcwd(), base_architecture_type + '_features.py'), dst=model_dir)\n","shutil.copy(src=os.path.join(os.getcwd(), 'model.py'), dst=model_dir)\n","shutil.copy(src=os.path.join(os.getcwd(), 'train_and_test.py'), dst=model_dir)\n","\n","log, logclose = create_logger(log_filename=os.path.join(model_dir, 'train.log'))\n","img_dir = os.path.join(model_dir, 'img')\n","makedir(img_dir) # Directory for saving prototypes\n","weight_matrix_filename = 'outputL_weights'\n","prototype_img_filename_prefix = 'prototype-img'\n","prototype_self_act_filename_prefix = 'prototype-self-act'\n","proto_bound_boxes_filename_prefix = 'bb'\n","\n","# load the data\n","from settings import train_dir, test_dir, train_push_dir\n","normalize = transforms.Normalize(mean=mean,\n","                                 std=std)\n","# all datasets\n","# train set\n","train_dataset = datasets.ImageFolder(\n","    train_dir,\n","    transforms.Compose([\n","        transforms.Resize(size=(img_size, img_size)),\n","        transforms.ToTensor(),\n","        normalize,\n","    ]))\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=train_batch_size, shuffle=False,\n","    num_workers=4, pin_memory=False)\n","# push set\n","train_push_dataset = datasets.ImageFolder(\n","    train_push_dir,\n","    transforms.Compose([\n","        transforms.Resize(size=(img_size, img_size)),\n","        transforms.ToTensor(),\n","    ]))\n","train_push_loader = torch.utils.data.DataLoader(\n","    train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n","    num_workers=4, pin_memory=False)\n","# test set\n","test_dataset = datasets.ImageFolder(\n","    test_dir,\n","    transforms.Compose([\n","        transforms.Resize(size=(img_size, img_size)),\n","        transforms.ToTensor(),\n","        normalize,\n","    ]))\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=test_batch_size, shuffle=False,\n","    num_workers=4, pin_memory=False)\n","# we should look into distributed sampler more carefully at torch.utils.data.distributed.DistributedSampler(train_dataset)\n","log('training set size: {0}'.format(len(train_loader.dataset)))\n","log('push set size: {0}'.format(len(train_push_loader.dataset)))\n","log('test set size: {0}'.format(len(test_loader.dataset)))\n","log('batch size: {0}'.format(train_batch_size))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aF8_Y-brBh08","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709339990107,"user_tz":-210,"elapsed":113287,"user":{"displayName":"MJ Lab","userId":"12526651074320095318"}},"outputId":"43a366bd-11d4-4827-816f-e44c0e1ee23c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[10  7  9 28  2  1  5  3]\n"," [ 8  5 10 18  4  1 10  1]\n"," [ 8  8  9 20  5  1  5  0]\n"," [14  5 10 22  4  2  2  2]\n"," [ 1  3  0  1 22 21 23 18]\n"," [ 0  2  0  1 26 20 26 27]\n"," [ 0  3  0  2 18 17 21 26]\n"," [ 1  5  0  0 21 21 25 15]]\n","              precision    recall  f1-score   support\n","\n","           0       0.24      0.15      0.19        65\n","           1       0.13      0.09      0.11        57\n","           2       0.24      0.16      0.19        56\n","           3       0.24      0.36      0.29        61\n","           4       0.22      0.25      0.23        89\n","           5       0.24      0.20      0.22       102\n","           6       0.18      0.24      0.21        87\n","           7       0.16      0.17      0.17        88\n","\n","    accuracy                           0.20       605\n","   macro avg       0.21      0.20      0.20       605\n","weighted avg       0.21      0.20      0.20       605\n","\n","All_labels: [array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7])]\n","All_preds: [array([0]), array([4]), array([3]), array([5]), array([1]), array([1]), array([3]), array([3]), array([3]), array([3]), array([1]), array([3]), array([2]), array([1]), array([0]), array([6]), array([3]), array([3]), array([2]), array([3]), array([2]), array([3]), array([0]), array([3]), array([3]), array([3]), array([1]), array([2]), array([3]), array([3]), array([3]), array([3]), array([0]), array([2]), array([3]), array([0]), array([3]), array([0]), array([1]), array([2]), array([0]), array([3]), array([0]), array([1]), array([0]), array([3]), array([3]), array([7]), array([7]), array([7]), array([4]), array([6]), array([6]), array([2]), array([3]), array([2]), array([3]), array([3]), array([3]), array([3]), array([0]), array([2]), array([6]), array([3]), array([6]), array([2]), array([6]), array([6]), array([4]), array([0]), array([3]), array([0]), array([0]), array([5]), array([4]), array([0]), array([6]), array([4]), array([3]), array([1]), array([1]), array([3]), array([1]), array([3]), array([0]), array([3]), array([2]), array([1]), array([0]), array([6]), array([2]), array([2]), array([2]), array([3]), array([3]), array([3]), array([6]), array([3]), array([3]), array([1]), array([3]), array([3]), array([3]), array([3]), array([0]), array([3]), array([7]), array([0]), array([6]), array([6]), array([6]), array([6]), array([4]), array([6]), array([2]), array([2]), array([3]), array([2]), array([2]), array([2]), array([3]), array([3]), array([4]), array([1]), array([2]), array([4]), array([4]), array([4]), array([2]), array([2]), array([3]), array([3]), array([3]), array([0]), array([3]), array([0]), array([3]), array([3]), array([2]), array([1]), array([3]), array([3]), array([0]), array([3]), array([3]), array([1]), array([2]), array([1]), array([1]), array([0]), array([1]), array([0]), array([2]), array([3]), array([0]), array([3]), array([1]), array([5]), array([6]), array([6]), array([0]), array([3]), array([6]), array([6]), array([4]), array([3]), array([6]), array([2]), array([1]), array([3]), array([0]), array([2]), array([2]), array([3]), array([3]), array([3]), array([3]), array([3]), array([6]), array([4]), array([0]), array([4]), array([3]), array([0]), array([0]), array([1]), array([2]), array([4]), array([2]), array([3]), array([3]), array([0]), array([0]), array([0]), array([3]), array([3]), array([2]), array([0]), array([0]), array([3]), array([3]), array([3]), array([3]), array([1]), array([0]), array([3]), array([3]), array([0]), array([0]), array([3]), array([0]), array([3]), array([3]), array([2]), array([0]), array([4]), array([6]), array([5]), array([7]), array([5]), array([7]), array([3]), array([3]), array([2]), array([2]), array([3]), array([3]), array([2]), array([2]), array([2]), array([1]), array([0]), array([1]), array([2]), array([3]), array([3]), array([1]), array([3]), array([3]), array([3]), array([5]), array([4]), array([4]), array([4]), array([5]), array([5]), array([4]), array([4]), array([5]), array([7]), array([7]), array([4]), array([6]), array([7]), array([5]), array([6]), array([5]), array([5]), array([7]), array([6]), array([7]), array([7]), array([5]), array([5]), array([7]), array([7]), array([7]), array([5]), array([5]), array([7]), array([5]), array([5]), array([4]), array([5]), array([7]), array([5]), array([5]), array([6]), array([5]), array([7]), array([4]), array([4]), array([7]), array([6]), array([6]), array([6]), array([7]), array([7]), array([4]), array([7]), array([6]), array([5]), array([6]), array([6]), array([4]), array([6]), array([1]), array([4]), array([1]), array([6]), array([0]), array([4]), array([7]), array([4]), array([5]), array([6]), array([4]), array([6]), array([4]), array([4]), array([4]), array([4]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([1]), array([4]), array([4]), array([4]), array([6]), array([6]), array([5]), array([7]), array([5]), array([6]), array([7]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([5]), array([7]), array([7]), array([5]), array([7]), array([4]), array([4]), array([4]), array([7]), array([5]), array([6]), array([7]), array([4]), array([5]), array([5]), array([5]), array([4]), array([7]), array([6]), array([7]), array([7]), array([7]), array([5]), array([7]), array([7]), array([5]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([7]), array([7]), array([7]), array([7]), array([4]), array([5]), array([4]), array([6]), array([6]), array([5]), array([4]), array([6]), array([6]), array([4]), array([7]), array([5]), array([4]), array([6]), array([6]), array([6]), array([6]), array([4]), array([6]), array([6]), array([7]), array([6]), array([4]), array([7]), array([7]), array([7]), array([6]), array([4]), array([6]), array([6]), array([4]), array([4]), array([6]), array([6]), array([6]), array([7]), array([6]), array([6]), array([6]), array([7]), array([6]), array([7]), array([6]), array([1]), array([5]), array([3]), array([4]), array([4]), array([4]), array([6]), array([5]), array([4]), array([1]), array([4]), array([7]), array([7]), array([7]), array([5]), array([6]), array([5]), array([4]), array([4]), array([4]), array([5]), array([4]), array([5]), array([7]), array([5]), array([7]), array([5]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([7]), array([4]), array([5]), array([7]), array([6]), array([5]), array([7]), array([5]), array([4]), array([5]), array([5]), array([7]), array([4]), array([4]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([7]), array([6]), array([5]), array([4]), array([5]), array([4]), array([4]), array([5]), array([6]), array([1]), array([6]), array([1]), array([3]), array([7]), array([4]), array([6]), array([4]), array([7]), array([5]), array([6]), array([6]), array([4]), array([6]), array([7]), array([5]), array([7]), array([7]), array([7]), array([3]), array([7]), array([6]), array([7]), array([7]), array([4]), array([4]), array([4]), array([4]), array([6]), array([6]), array([1]), array([6]), array([7]), array([7]), array([5]), array([1]), array([5]), array([5]), array([5]), array([7]), array([4]), array([4]), array([4]), array([5]), array([5]), array([4]), array([7]), array([7]), array([4]), array([4]), array([7]), array([6]), array([5]), array([7]), array([5]), array([4]), array([5]), array([5]), array([5]), array([7]), array([4]), array([4]), array([6]), array([6]), array([7]), array([6]), array([6]), array([7]), array([7]), array([6]), array([5]), array([5]), array([7]), array([5]), array([4]), array([4]), array([4]), array([5]), array([4]), array([0]), array([4]), array([1]), array([6]), array([4]), array([1]), array([4]), array([6]), array([7]), array([4]), array([7]), array([6]), array([7]), array([6]), array([5]), array([1]), array([4]), array([5]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([6]), array([7]), array([6]), array([4]), array([1]), array([6]), array([4]), array([4]), array([6]), array([5]), array([5]), array([6]), array([6]), array([6]), array([5]), array([5]), array([5]), array([7])]\n","Counts(b & m): [239, 366]\n"]}],"source":["# Test with 2*n_cluster pseudo-classes\n","#from settings import num_classes\n","dataloader = test_loader\n","\n","#model = model.cuda()\n","All_preds = []\n","All_labels = []\n","model.eval()\n","#s_b = [0, 1, 2, 3, 4]\n","#s_m = [5, 6, 7, 8, 9]\n","#s_b = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 23, 34, 35, 36, 37, 38, 39]\n","#s_m = [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n","c_b = 0\n","c_m = 0\n","count = torch.zeros(40)\n","for i, (image, label) in enumerate(dataloader):\n","    input = image.cuda()\n","    target = label.cuda()\n","    count[label] += 1\n","\n","    output, min_distances = model(input)\n","    _, predicted_0 = torch.max(output.data, 1)\n","    predicted = predicted_0.cpu().numpy().astype(int)\n","\n","    #if target in s_b:  # # for original two classes in test\n","    if target < num_classes/2:\n","       c_b += 1\n","    else:\n","       c_m += 1\n","\n","    All_preds.append(predicted)\n","    All_labels.append(target.cpu().numpy().astype(int))\n","\n","    #c += 1\n","    #if c == 5:\n","    #   break\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","print(confusion_matrix(All_labels ,All_preds ))\n","print(classification_report(All_labels ,All_preds ))\n","#torch.save(All_preds, 'All_preds_10.pt')\n","#torch.save(All_labels, 'All_labels.pt')\n","print('All_labels:', All_labels)\n","print('All_preds:', All_preds)\n","print('Counts(b & m):', [c_b, c_m])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nd_Ovhb6x-93","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709340000771,"user_tz":-210,"elapsed":10667,"user":{"displayName":"MJ Lab","userId":"12526651074320095318"}},"outputId":"bf2c85db-ad47-48fa-fde4-9821f85623eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[191  48]\n"," [ 19 347]]\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.80      0.85       239\n","           1       0.88      0.95      0.91       366\n","\n","    accuracy                           0.89       605\n","   macro avg       0.89      0.87      0.88       605\n","weighted avg       0.89      0.89      0.89       605\n","\n","All_labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","All_preds: [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Counts(b & m): [239, 366]\n"]}],"source":["# Test_acc with binarized pseudo_classes (remapped to B and M)\n","\n","dataloader = test_loader  # train_loader  #\n","\n","#model = model.cuda()\n","All_preds = []\n","All_labels = []\n","All_preds_2 = []\n","All_labels_2 = []\n","model.eval()\n","#s_b = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 23, 34, 35, 36, 37, 38, 39]\n","#s_m = [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n","c_b = 0\n","c_m = 0\n","count = torch.zeros(40)\n","for i, (image, label) in enumerate(dataloader):\n","    input = image.cuda()\n","    target_0 = label.cuda()\n","    count[label] += 1\n","\n","    output, min_distances = model(input)\n","    _, predicted_0 = torch.max(output.data, 1)\n","    predicted_0 = predicted_0.cpu().numpy().astype(int)\n","    #predicted = predicted_0  # for 40 new classes in test\n","    #if predicted_0 in s_b:  # # for original two classes in test\n","    if predicted_0 < num_classes/2:\n","       predicted = 0\n","    else:\n","       predicted = 1\n","\n","    #if target_0 in s_b:  # # for original two classes in test\n","    if target_0 < num_classes/2:\n","       target = 0\n","       c_b += 1\n","    else:\n","       target = 1\n","       c_m += 1\n","\n","    All_preds.append(predicted)\n","    All_labels.append(target)\n","\n","    All_preds_2.append(predicted_0)\n","    All_labels_2.append(target_0)\n","\n","    #c += 1\n","    #if c == 5:\n","    #   break\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","print(confusion_matrix(All_labels ,All_preds ))\n","print(classification_report(All_labels ,All_preds ))\n","#torch.save(All_preds, 'All_preds_10.pt')\n","#torch.save(All_labels, 'All_labels.pt')\n","print('All_labels:', All_labels)\n","print('All_preds:', All_preds)\n","#import numpy as np\n","#print('difference:', np.array(All_labels) - np.array(All_preds))\n","print('Counts(b & m):', [c_b, c_m])"]},{"cell_type":"code","source":["# Train_acc with binarized pseudo_classes (remapped to B and M)\n","\n","dataloader = train_loader  # train_loader  #\n","\n","#model = model.cuda()\n","All_preds = []\n","All_labels = []\n","All_preds_2 = []\n","All_labels_2 = []\n","model.eval()\n","#s_b = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 23, 34, 35, 36, 37, 38, 39]\n","#s_m = [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n","c_b = 0\n","c_m = 0\n","count = torch.zeros(40)\n","for i, (image, label) in enumerate(dataloader):\n","    input = image.cuda()\n","    target_0 = label.cuda()\n","    count[label] += 1\n","\n","    output, min_distances = model(input)\n","    _, predicted_0 = torch.max(output.data, 1)\n","    predicted_0 = predicted_0.cpu().numpy().astype(int)\n","    #predicted = predicted_0  # for 40 new classes in test\n","    #if predicted_0 in s_b:  # # for original two classes in test\n","    if predicted_0 < num_classes/2:\n","       predicted = 0\n","    else:\n","       predicted = 1\n","\n","    #if target_0 in s_b:  # # for original two classes in test\n","    if target_0 < num_classes/2:\n","       target = 0\n","       c_b += 1\n","    else:\n","       target = 1\n","       c_m += 1\n","\n","    All_preds.append(predicted)\n","    All_labels.append(target)\n","\n","    All_preds_2.append(predicted_0)\n","    All_labels_2.append(target_0)\n","\n","    #c += 1\n","    #if c == 5:\n","    #   break\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","print(confusion_matrix(All_labels ,All_preds ))\n","print(classification_report(All_labels ,All_preds ))\n","#torch.save(All_preds, 'All_preds_10.pt')\n","#torch.save(All_labels, 'All_labels.pt')\n","print('All_labels:', All_labels)\n","print('All_preds:', All_preds)\n","#import numpy as np\n","#print('difference:', np.array(All_labels) - np.array(All_preds))\n","print('Counts(b & m):', [c_b, c_m])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mbVKpP_cKTtd","outputId":"5e0f7df4-1c61-4a85-a9b1-a8ccf16ccc6c","executionInfo":{"status":"ok","timestamp":1709325888744,"user_tz":-210,"elapsed":178917,"user":{"displayName":"MJ Lab","userId":"12526651074320095318"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[4792    8]\n"," [   8 4792]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      4800\n","           1       1.00      1.00      1.00      4800\n","\n","    accuracy                           1.00      9600\n","   macro avg       1.00      1.00      1.00      9600\n","weighted avg       1.00      1.00      1.00      9600\n","\n","All_labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","All_preds: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Counts(b & m): [4800, 4800]\n"]}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}